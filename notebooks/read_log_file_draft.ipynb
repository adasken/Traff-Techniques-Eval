{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files\n",
    "f = open('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/13898-STGCN-PEMS_BAY_0.5-May-28-2022_13-25-32.log', 'r')\n",
    "# store the lines and close the files\n",
    "lines = f.read().splitlines()\n",
    "f.close()\n",
    "\n",
    "# turn the file into dataframe\n",
    "df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "# get rows that start the training\n",
    "start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "# extract lines with Epoch info\n",
    "info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "# statistics\n",
    "stats = {\n",
    "    'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "    'val_loss_min': info['val_loss'].min(),\n",
    "    'train_loss_min': info['train_loss'].min(),\n",
    "    'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "    'num_epoch': info.shape[0],\n",
    "    'time_taken_mean': info['time_taken'].mean(),\n",
    "    'time_taken_median': info['time_taken'].median(),\n",
    "    'time_taken_max': info['time_taken'].max(),\n",
    "    'time_taken_min': info['time_taken'].min(),\n",
    "    'time_taken_std': info['time_taken'].std()\n",
    "}\n",
    "\n",
    "pd.DataFrame.from_dict(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter-nbconvert --to webpdf read_log_file.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'adaskenww'\n",
    "!jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../libcity/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = os.listdir('../libcity/log')\n",
    "logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = os.listdir('../libcity/log')\n",
    "logs[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = os.listdir('../libcity/log')\n",
    "logs[0].split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "log_names[0].split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "log_names[0].split('-')\n",
    "for log in log_names:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "log_names[0].split('-')\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "pattern = re.compile('\\d')\n",
    "pattern.match(t)# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "pattern = re.compile('\\d')\n",
    "pattern.match(t[0])\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "pattern = re.compile('\\d+')\n",
    "pattern.match(t[0])\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "\n",
    "re.match(r'\\d+', t[0]\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "\n",
    "re.match(r'\\d+', t[0])\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "\n",
    "bool(re.match(r'\\d+', t[0]))\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "\n",
    "bool(re.match(r'\\d+', t[4]))\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "\n",
    "bool(re.match(r'\\d+', t[4]))\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)\n",
    "t[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "\n",
    "bool(re.match(r'\\d+', t[4]))\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[4].split('-')\n",
    "\n",
    "bool(re.match(r'\\d+', t[4]))\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[4].split('-')\n",
    "\n",
    "bool(re.match(r'\\d+', t[0]))\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[4].split('-')\n",
    "\n",
    "bool(re.match(r'\\d+', t[0]))\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[4].split('-')\n",
    "if re.match(r'\\d+', t[0]):\n",
    "    print('yes')\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "if re.match(r'\\d+', t[0]):\n",
    "    print('yes')\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "if re.match(r'\\d+-', t[0]):\n",
    "    \n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "if re.match(r'\\d+-', t[0]):\n",
    "    print('yes')\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "if re.match(r'\\d+-', t[0]):\n",
    "    print('yes')\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "if re.match(r'\\d+\\-', t[0]):\n",
    "    print('yes')\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "t = log_names[0].split('-')\n",
    "if re.match(r'\\d+', t[0]):\n",
    "    print('yes')\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '/sd/adaskenww'\n",
    "!jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '/sd/adaskenww'\n",
    "!jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'sd/adaskenww'\n",
    "!jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', t[0]):\n",
    "        print(log_detail)\n",
    "\n",
    "# for log in log_names:\n",
    "#     print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postfix_path='../libcity/log'\n",
    "\n",
    "def read_log():\n",
    "\n",
    "    # read the files\n",
    "    f = open('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/13898-STGCN-PEMS_BAY_0.5-May-28-2022_13-25-32.log', 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    pd.DataFrame.from_dict(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postfix_path='../libcity/log'\n",
    "\n",
    "def read_log():\n",
    "\n",
    "    # read the files\n",
    "    f = open('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/13898-STGCN-PEMS_BAY_0.5-May-28-2022_13-25-32.log', 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_names = os.listdir('../libcity/log')\n",
    "logs_dict = {}\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', t[0]):\n",
    "        read_log()\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "log_names = os.listdir('../libcity/log')\n",
    "logs_dict = {}\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', t[0]):\n",
    "        read_log()\n",
    "        clear_output(wait=True)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postfix_path='../libcity/log'\n",
    "\n",
    "def read_log():\n",
    "\n",
    "    # read the files\n",
    "    f = open('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/13898-STGCN-PEMS_BAY_0.5-May-28-2022_13-25-32.log', 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "log_names = os.listdir('../libcity/log')\n",
    "logs_dict = {}\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', t[0]):\n",
    "        read_log()\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postfix_path='../libcity/log'\n",
    "\n",
    "def read_log():\n",
    "\n",
    "    # read the files\n",
    "    f = open('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/13898-STGCN-PEMS_BAY_0.5-May-28-2022_13-25-32.log', 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "log_names = os.listdir('../libcity/log')\n",
    "logs_dict = {}\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', t[0]):\n",
    "        read_log()\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', t[0]):\n",
    "        print(log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "i = 1\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        i=i+1\n",
    "        print(i)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "#         read_log()\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "    print(info)\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'sd/adaskenww'\n",
    "!jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "    print(info['line'].apply(lambda x: x.split())\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "    print(info['line'].apply(lambda x: x.split()))\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    print(info['line'].apply(lambda x: x.split()))\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'sd/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    print(info['line'].apply(lambda x: x.split()))\n",
    "    \n",
    "    \n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'sd/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    print(info['line'].apply(lambda x: x.split())[0])\n",
    "    \n",
    "    \n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'sd/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    print(info['line'][0].split())\n",
    "    \n",
    "    \n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[6])\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[8][:-1]))\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[10][:-1]))\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[12][:-1]))\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[13][:-1]))\n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'sd/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        print(log_path)\n",
    "        read_log(log_path)\n",
    "        break\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '2022-05-27 00:10:44,089 - INFO - Epoch [5/100] (3420) train_loss: 0.8156, val_loss: 1.8024, lr: 0.001000, 1124.72s'\n",
    "\n",
    "re.search('((?<=train_loss:\\s+).*)(?=\\,\\s+val_loss)', s).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '2022-05-27 00:10:44,089 - INFO - Epoch [5/100] (3420) train_loss: 0.8156, val_loss: 1.8024, lr: 0.001000, 1124.72s'\n",
    "\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '2022-05-27 00:10:44,089 - INFO - Epoch [5/100] (3420) train_loss: 0.8156, val_loss: 1.8024, lr: 0.001000, 1124.72s'\n",
    "\n",
    "s = s.split()\n",
    "\n",
    "[i for i, s in enumerate(s) if 'train_loss:' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '2022-05-27 00:10:44,089 - INFO - Epoch [5/100] (3420) train_loss: 0.8156, val_loss: 1.8024, lr: 0.001000, 1124.72s'\n",
    "\n",
    "s = s.split()\n",
    "\n",
    "print(s)\n",
    "\n",
    "[i for i, s in enumerate(s) if 'train_loss:' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = line_split[index[0]+1]  \n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr::' in s]\n",
    "    info['lr'] = line_split[index[0]+1]\n",
    "    \n",
    "    info['time_taken'] = line_split[-1]\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = line_split[index[0]+1]  \n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = line_split[index[0]+1]\n",
    "    \n",
    "    info['time_taken'] = line_split[-1]\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = line_split[index[0]+1]  \n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = line_split[index[0]+1]\n",
    "    \n",
    "    info['time_taken'] = line_split[-1]\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = line_split[index[0]+1]  \n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = line_split[index[0]+1]\n",
    "    \n",
    "    info['time_taken'] = line_split[-1]\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = line_split[index[0]+1]  \n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = line_split[index[0]+1]\n",
    "    \n",
    "    info['time_taken'] = line_split[-1]\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = line_split[index[0]+1]  \n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = line_split[index[0]+1]\n",
    "    \n",
    "    info['time_taken'] = line_split[-1]\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1][:-1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = float(line_split[index[0]+1])\n",
    "    \n",
    "    info['time_taken'] = float(line_split[-1][:-1])\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1][:-1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    info['time_taken'] = float(line_split[-1][:-1])\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    info['time_taken'] = float(line_split[-1][:-1])\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    info['time_taken'] = float(line_split[-1][:-1])\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = line_split[index[0]+1]\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    info['time_taken'] = float(line_split[-1][:-1])\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[line_split[index[0]+1]])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    info['time_taken'] = float(line_split[-1][:-1])\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = float(line_split[index[0]+1][:-1])\n",
    "    \n",
    "    info['time_taken'] = float(line_split[-1][:-1])\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[index[0]+1]))\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "#     info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "#     info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "#     info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "#     stats = {\n",
    "#         'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "#         'val_loss_min': info['val_loss'].min(),\n",
    "#         'train_loss_min': info['train_loss'].min(),\n",
    "#         'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "#         'num_epoch': info.shape[0],\n",
    "#         'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "#         'time_taken_max': info['time_taken'].max(),\n",
    "#         'time_taken_min': info['time_taken'].min(),\n",
    "#         'time_taken_std': info['time_taken'].std()\n",
    "#     }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/36442-RNN-PEMS_BAY_0.5-May-25-2022_20-08-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/36442-RNN-PEMS_BAY_0.5-May-25-2022_20-08-15.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "#     display(pd.DataFrame.from_dict(stats))\n",
    "#     plt.show()\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/36442-RNN-PEMS_BAY_0.5-May-25-2022_20-08-15.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'sd/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/36442-RNN-PEMS_BAY_0.5-May-25-2022_20-08-15.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/36442-RNN-PEMS_BAY_0.5-May-25-2022_20-08-15.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        read_log(log_path)\n",
    "        break\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "#     print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "#     print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        log_path = postfix_path + log_name\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        log_path = postfix_path + log_name\n",
    "        read_log(log_path)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "class bcolors:\n",
    "    OK = '\\033[92m\\033[1m' #GREEN\n",
    "    WARNING = '\\033[93m\\033[1m' #YELLOW\n",
    "    FAIL = '\\033[91m' #RED\n",
    "    RESET = '\\033[0m' #RESET COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "#     print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    \n",
    "    %%javascript\n",
    "    IPython.notebook.save_notebook()\n",
    "\n",
    "    name = 'result/' + file_name\n",
    "    !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'result/adaskenww'\n",
    "!jupyter-nbconvert --to html --output {name} read_log_file.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "#     print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    \n",
    "    %%javascript\n",
    "    IPython.notebook.save_notebook()\n",
    "\n",
    "    name = 'result/' + file_name\n",
    "    !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "read_log('65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$(\"#save-notbook button\").trigger('click');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "%%javascript\n",
    "$(\"#save-notbook button\").trigger('click');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "\n",
    "def notebook_save():\n",
    "    Javascript(script)\n",
    "    print('This notebook has been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Javascript\n",
    "\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "\n",
    "def notebook_save():\n",
    "    Javascript(script)\n",
    "    print('This notebook has been saved')\n",
    "    \n",
    "class bcolors:\n",
    "    OK = '\\033[92m\\033[1m' #GREEN\n",
    "    WARNING = '\\033[93m\\033[1m' #YELLOW\n",
    "    FAIL = '\\033[91m' #RED\n",
    "    RESET = '\\033[0m' #RESET COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "#     print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    \n",
    "    %%javascript\n",
    "    IPython.notebook.save_notebook()\n",
    "\n",
    "    name = 'result/' + file_name\n",
    "    !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Javascript\n",
    "\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "\n",
    "def notebook_save():\n",
    "    Javascript(script)\n",
    "    print('This notebook has been saved')\n",
    "    \n",
    "class bcolors:\n",
    "    OK = '\\033[92m\\033[1m' #GREEN\n",
    "    WARNING = '\\033[93m\\033[1m' #YELLOW\n",
    "    FAIL = '\\033[91m' #RED\n",
    "    RESET = '\\033[0m' #RESET COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "            name = 'result/' + log_detail[0]\n",
    "            Javascript(script)\n",
    "            !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "            clear_output(wait=True)\n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        log_path = postfix_path + log_name\n",
    "        read_log(log_path)\n",
    "        name = 'result/' + log_detail[0]\n",
    "        Javascript(script)\n",
    "        !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "        clear_output(wait=True)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "#     print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    \n",
    "#     name = 'result/' + file_name\n",
    "#     !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "    \n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "            name = 'result/' + file_name\n",
    "            Javascript(script)\n",
    "            !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "            clear_output(wait=True)            \n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "#     print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    \n",
    "#     name = 'result/' + file_name\n",
    "#     !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "    \n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'result/adaskenww'\n",
    "# !jupyter-nbconvert --to html --output {name} read_log_file.ipynb \n",
    "\n",
    "# read_log('/Users/lunaryk/Desktop/Traffic-Demand-Evaluation/Bigscity-LibCity/libcity/log/65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log')\n",
    "\n",
    "notebook_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "            name = 'result/' + file_name\n",
    "            Javascript(script)\n",
    "            !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "            clear_output(wait=True)            \n",
    "\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        log_path = postfix_path + log_name\n",
    "        read_log(log_path)\n",
    "        name = 'result/' + file_name\n",
    "        Javascript(script)\n",
    "        !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "        clear_output(wait=True)            \n",
    "\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "            name = 'result/' + log_name\n",
    "            !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "            clear_output(wait=True)            \n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log(file_name):\n",
    "\n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "#     print(line_split)\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    \n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken')\n",
    "    info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss')\n",
    "    info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss')\n",
    "\n",
    "    # statistics\n",
    "    stats = {\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'val_loss_min': info['val_loss'].min(),\n",
    "        'train_loss_min': info['train_loss'].min(),\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "        'num_epoch': info.shape[0],\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "        'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std()\n",
    "    }\n",
    "\n",
    "    display(pd.DataFrame.from_dict(stats))\n",
    "    plt.show()\n",
    "    \n",
    "#     name = 'result/' + file_name\n",
    "#     !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "    \n",
    "#     clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "            name = 'result/' + log_name\n",
    "            Javascript(script)\n",
    "            !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "            clear_output(wait=True)            \n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        print(log_name)\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path)\n",
    "            name = 'result/' + log_name\n",
    "            Javascript(script)\n",
    "            !jupyter-nbconvert --to html --output {name} read_log_file.ipynb  \n",
    "            clear_output(wait=True)            \n",
    "        except:\n",
    "            print('Error', log_name)\n",
    "        # call the other function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    Javascript(script)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    Javascript(script)\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(i)\n",
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook -e foo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(i)\n",
    "    %notebook -e read_log_file.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
