{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Javascript\n",
    "from datetime import datetime\n",
    "\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "\n",
    "def notebook_save():\n",
    "    Javascript(script)\n",
    "    print('This notebook has been saved')\n",
    "    \n",
    "class bcolors:\n",
    "    OK = '\\033[92m\\033[1m' #GREEN\n",
    "    WARNING = '\\033[93m\\033[1m' #YELLOW\n",
    "    FAIL = '\\033[91m' #RED\n",
    "    RESET = '\\033[0m' #RESET COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_log(file_name, model_name=None, dataset=None):\n",
    "    \n",
    "    # read the files\n",
    "    f = open(file_name, 'r')\n",
    "    # store the lines and close the files\n",
    "    lines = f.read().splitlines()\n",
    "    f.close()\n",
    "\n",
    "    # turn the file into dataframe\n",
    "    df = pd.DataFrame(lines, columns = ['line'])\n",
    "\n",
    "    # get rows that start and end the training\n",
    "    start_training_row_no = df[df['line'].str.contains('Start training') == True].index.tolist()[0]\n",
    "    end_training_row_no = df[df['line'].str.contains('Trained totally') == True].index.tolist()[0]\n",
    "\n",
    "    # extract lines with Epoch info\n",
    "    info = df[df['line'].str.contains('Epoch \\[.*\\]') == True].reset_index(drop=True)\n",
    "\n",
    "    line_split = info['line'][0].split()\n",
    "    \n",
    "    # Get detail from the line\n",
    "    index = [i for i, s in enumerate(line_split) if 'Epoch' in s]\n",
    "    info['epoch'] = info['line'].apply(lambda x: x.split()[index[0]+1])\n",
    "    # Get the train loss\n",
    "    index = [i for i, s in enumerate(line_split) if 'train_loss:' in s]\n",
    "    info['train_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    # Get the val loss\n",
    "    index = [i for i, s in enumerate(line_split) if 'val_loss:' in s]\n",
    "    info['val_loss'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    # Get the learning rate\n",
    "    index = [i for i, s in enumerate(line_split) if 'lr:' in s]\n",
    "    info['lr'] = info['line'].apply(lambda x: float(x.split()[index[0]+1][:-1]))\n",
    "    # Get the time taken\n",
    "    info['time_taken'] = info['line'].apply(lambda x: float(x.split()[-1][:-1]))\n",
    "    \n",
    "    # generate figures\n",
    "    path = 'result' + '/' + model_name + '/'\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except FileExistsError:\n",
    "        pass    \n",
    "    fig_epoch_timeTaken = info.plot.line(x='epoch', y='time_taken',figsize=(10, 5), title='epoch and time_taken').get_figure()\n",
    "    fig_epoch_timeTaken.savefig(path + dataset + \"_timeTaken_\" + model_name + '.jpg')\n",
    "    plt.close()\n",
    "    fig_epoch_valLoss = info.plot.line(x='epoch', y='val_loss',figsize=(10, 5), title='epoch and val_loss').get_figure()\n",
    "    fig_epoch_valLoss.savefig(path + dataset + \"_valLoss_\" + model_name + '.jpg')\n",
    "    plt.close()\n",
    "    fig_epoch_trainLoss = info.plot.line(x='epoch', y='train_loss',figsize=(10, 5), title='epoch and train_loss').get_figure()\n",
    "    fig_epoch_trainLoss.savefig(path + dataset + \"_trainLoss_\" + model_name + '.jpg')\n",
    "    plt.close()\n",
    "    \n",
    "    # statistics\n",
    "    stats = {\n",
    "        'time_taken_mean': info['time_taken'].mean(),\n",
    "#         'time_taken_median': info['time_taken'].median(),\n",
    "        'time_taken_max': info['time_taken'].max(),\n",
    "        'time_taken_min': info['time_taken'].min(),\n",
    "        'time_taken_std': info['time_taken'].std(),\n",
    "        'val_loss_from': info['val_loss'][0],\n",
    "        'val_loss_to': info['val_loss'].min(),\n",
    "        'train_loss_from': info['train_loss'][0],\n",
    "        'train_loss_to': info['train_loss'].min(),\n",
    "        'max_epoch': int(re.search('\\d+(?=\\])', info['epoch'][0]).group(0)),\n",
    "        'num_epoch': info.shape[0],\n",
    "        'train_loss_min_index': [info[info['train_loss'] == info['train_loss'].min()].index.tolist()],\n",
    "    }\n",
    "    # create a dataframe for exprting\n",
    "    df_stats = pd.DataFrame.from_dict(stats)\n",
    "    stats = df_stats.to_string()\n",
    "    # save to text file\n",
    "    text_file = open(path + dataset + '_' + model_name + \"_stats.txt\", \"w\")\n",
    "    n = text_file.write(stats)\n",
    "    text_file.close()\n",
    "    \n",
    "    # datetime report\n",
    "    datetime_start_training = lines[start_training_row_no].split()[0]+ ' ' +lines[start_training_row_no].split()[1]\n",
    "    datetime_object_start_training = datetime.strptime(datetime_start_training, '%Y-%m-%d %H:%M:%S,%f')\n",
    "    datetime_end_training = lines[end_training_row_no].split()[0]+ ' ' +lines[end_training_row_no].split()[1]\n",
    "    datetime_object_end_training = datetime.strptime(datetime_end_training, '%Y-%m-%d %H:%M:%S,%f')\n",
    "    # calculate the duration of training\n",
    "    total_train_time = datetime_object_end_training - datetime_object_start_training\n",
    "    total_train_time_in_second = total_train_time.total_seconds()\n",
    "    total_train_time_in_minute = divmod(total_train_time_in_second, 60)[0]\n",
    "    total_train_time_in_hour = divmod(total_train_time_in_second, 3600)[0]\n",
    "    # save to the file\n",
    "    text_file = open(path + dataset + '_' + model_name + \"_stats.txt\", \"a\")\n",
    "    text_file.write('\\n\\nTotal training time (hrs): ' + str(total_train_time_in_hour) + ' hours')\n",
    "    text_file.write('\\nTotal training time (sec): ' + str(total_train_time_in_second) + ' seconds')\n",
    "    text_file.write('\\nTotal training time (min): ' + str(total_train_time_in_minute) + ' minutes')\n",
    "    text_file.write('\\nStart training time: ' + str(datetime_start_training))\n",
    "    text_file.write('\\nStart training time: ' + str(datetime_end_training))\n",
    "    text_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m58804-RNN-PEMS_BAY-May-25-2022_22-12-20.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m26362-DCRNN-PEMS_BAY_0.5-May-26-2022_22-20-38.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m97760-STGCN-PEMS_BAY-May-28-2022_04-43-36.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m65598-RNN-PEMS_BAY_0.01-Jun-07-2022_12-26-51.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m58371-MTGNN-PEMS_BAY-Jun-01-2022_06-16-52.log loaded successfully\u001b[0m\n",
      "\u001b[93m\u001b[1m23573-CCRNN-PEMS_BAY_0.5-Jun-01-2022_06-12-46.log failed to load\u001b[0m\n",
      "\u001b[93m\u001b[1m47407-TGCN-PEMS_BAY_0.5-Jun-08-2022_10-08-29.log failed to load\u001b[0m\n",
      "\u001b[92m\u001b[1m37415-RNN-PEMS_BAY_0.5-May-28-2022_03-19-53.log loaded successfully\u001b[0m\n",
      "\u001b[93m\u001b[1m52425-CCRNN-PEMS_BAY-Jun-01-2022_06-04-00.log failed to load\u001b[0m\n",
      "\u001b[92m\u001b[1m36442-RNN-PEMS_BAY_0.5-May-25-2022_20-08-15.log loaded successfully\u001b[0m\n",
      "\u001b[93m\u001b[1m93691-DCRNN-PEMS_BAY_0.5-May-26-2022_22-00-29.log failed to load\u001b[0m\n",
      "\u001b[92m\u001b[1m45665-GWNET-PEMS_BAY-May-28-2022_16-44-07.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m58075-RNN-METR_LA-May-06-2022_21-56-29.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m13898-STGCN-PEMS_BAY_0.5-May-28-2022_13-25-32.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m38011-TGCN-PEMS_BAY-Jun-01-2022_13-14-37.log loaded successfully\u001b[0m\n",
      "\u001b[93m\u001b[1m89867-RNN-PEMS_BAY-May-26-2022_22-16-31.log failed to load\u001b[0m\n",
      "\u001b[92m\u001b[1m71743-RNN-PEMS_BAY-May-28-2022_02-31-12.log loaded successfully\u001b[0m\n",
      "\u001b[92m\u001b[1m51181-GWNET-PEMS_BAY_0.5-May-30-2022_13-26-58.log loaded successfully\u001b[0m\n",
      "\u001b[93m\u001b[1m41296-DCRNN-PEMS_BAY-May-26-2022_22-00-05.log failed to load\u001b[0m\n",
      "\u001b[92m\u001b[1m70846-STGCN-LOS_LOOP-May-07-2022_17-59-24.log loaded successfully\u001b[0m\n",
      "\u001b[93m\u001b[1m55958-DCRNN-PEMS_BAY_0.5-May-26-2022_22-13-53.log failed to load\u001b[0m\n",
      "\u001b[92m\u001b[1m24146-MTGNN-PEMS_BAY_0.5-Jun-01-2022_10-18-58.log loaded successfully\u001b[0m\n",
      "\u001b[93m\u001b[1m94782-DCRNN-PEMS_BAY-May-26-2022_22-17-46.log failed to load\u001b[0m\n",
      "\u001b[93m\u001b[1m42244-DCRNN-PEMS_BAY-May-26-2022_22-13-29.log failed to load\u001b[0m\n",
      "\u001b[93m\u001b[1m60191-RNN-LOS_LOOP-May-07-2022_17-58-53.log failed to load\u001b[0m\n",
      "{'success': 15, 'failed': 10, 'total': 25}\n"
     ]
    }
   ],
   "source": [
    "postfix_path='../libcity/log/'\n",
    "log_names = os.listdir('../libcity/log')\n",
    "processed_files = {\n",
    "    'success': 0,\n",
    "    'failed': 0,\n",
    "    'total': 0\n",
    "}\n",
    "for log_name in log_names:\n",
    "    \n",
    "    log_detail = log_name.split('-')\n",
    "    if re.match(r'\\d+', log_detail[0]):\n",
    "        try:\n",
    "            log_path = postfix_path + log_name\n",
    "            read_log(log_path, log_detail[1], log_detail[2])        \n",
    "        except:\n",
    "            print(bcolors.WARNING + log_name, \"failed to load\" + bcolors.RESET)\n",
    "            processed_files['failed']+=1\n",
    "        else:\n",
    "            print(bcolors.OK + log_name, \"loaded successfully\" + bcolors.RESET)\n",
    "            processed_files['success']+=1\n",
    "        finally:\n",
    "            processed_files['total']+=1\n",
    "            \n",
    "print(processed_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
